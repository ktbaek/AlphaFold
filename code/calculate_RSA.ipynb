{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "union-finish",
   "metadata": {},
   "source": [
    "Python Notebook by Kristoffer T. BÃ¦k, 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from Bio import PDB, SeqIO\n",
    "from Bio.PDB import PDBList, PDBParser, parse_pdb_header\n",
    "from Bio import pairwise2\n",
    "import freesasa as fs\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-complaint",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residueAreas(result):\n",
    "    '''Extracts output values from freesasa and calculates RSA.'''            \n",
    "\n",
    "    aa_codes = {'ALA': 'A',\n",
    "             'CYS': 'C',\n",
    "             'ASP': 'D',\n",
    "             'GLU': 'E',\n",
    "             'PHE': 'F',\n",
    "             'GLY': 'G',\n",
    "             'HIS': 'H',\n",
    "             'ILE': 'I',\n",
    "             'LYS': 'K',\n",
    "             'LEU': 'L',\n",
    "             'MET': 'M',\n",
    "             'ASN': 'N',\n",
    "             'PRO': 'P',\n",
    "             'GLN': 'Q',\n",
    "             'ARG': 'R',\n",
    "             'SER': 'S',\n",
    "             'THR': 'T',\n",
    "             'VAL': 'V',\n",
    "             'TRP': 'W',\n",
    "             'TYR': 'Y'}\n",
    "\n",
    "    l = []\n",
    "    r = result.residueAreas()\n",
    "    Sasa = namedtuple('Sasa', ['chain', 'number', 'residue', 'total_abs', 'total_rel', 'main_abs', 'main_rel'])\n",
    "    \n",
    "    for chain, chainvalue in r.items():\n",
    "        for residue, value in chainvalue.items():\n",
    "            if len(value.residueType) == 3: # disregard nucleotides\n",
    "                residue_code = aa_codes[value.residueType]\n",
    "                l.append(Sasa(chain,\n",
    "                          str(value.residueNumber),\n",
    "                          residue_code,\n",
    "                          value.total,\n",
    "                          value.relativeTotal,\n",
    "                          value.mainChain,\n",
    "                          value.relativeMainChain\n",
    "                         ))\n",
    "\n",
    "    return l\n",
    "\n",
    "def calc_RSA(filepath):\n",
    "    '''Runs freesasa on PDB.'''\n",
    "    \n",
    "    classifier = fs.Classifier.getStandardClassifier('naccess')\n",
    "    structure = fs.Structure(filepath, classifier)\n",
    "    result = fs.calc(structure)\n",
    "    \n",
    "    return get_residueAreas(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaps(sequence):\n",
    "     '''Identify gaps from alignments.'''\n",
    "\n",
    "    for i, res in enumerate(sequence):\n",
    "        if res not in ['-', 'X'] or i == len(sequence) - 1:\n",
    "            gap_start = i\n",
    "            break\n",
    "    for i, res in enumerate(sequence[::-1]):\n",
    "        if res not in ['-', 'X'] or i == len(sequence) - 1:\n",
    "            gap_end = i\n",
    "            break\n",
    "    gap_mid = []\n",
    "    gap = False\n",
    "    end = len(sequence) - gap_end\n",
    "    for i, res in enumerate(sequence[gap_start : end]):\n",
    "        if res in ['-', 'X'] and not gap:\n",
    "            start = i\n",
    "            gap = True\n",
    "        if res not in ['-', 'X'] and gap:\n",
    "            end = i\n",
    "            gap = False\n",
    "            gap_mid.append(end - start)\n",
    "    \n",
    "    Gaps = namedtuple(\"Gaps\", [\"start\", \"mid\", \"end\"])\n",
    "    \n",
    "    return Gaps(gap_start, gap_mid, gap_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_RSA_pair(pair):\n",
    "    '''Calculates RSA from a pair of AF and experimental PDBs.'''\n",
    "    \n",
    "    # paths to PDB files\n",
    "    af_path = '../data/external/AF_human/' + pair.af_name\n",
    "    pdb_path = '../data/external/PDB/pdb' + pair.chain_name[:4].lower() + '.ent'\n",
    "        \n",
    "    chain = pair.chain_name[5]\n",
    "    \n",
    "    rsa_af = calc_RSA(af_path)\n",
    "    rsa_pdb = calc_RSA(pdb_path)\n",
    "    \n",
    "    rsa_chain = [item for item in rsa_pdb if item.chain == chain]\n",
    "    num_chains = len(set([item.chain for item in rsa_pdb]))\n",
    "    \n",
    "    # check that there are still no mid-gaps or mid-insertions when extracting sequences with freesasa\n",
    "    seq_af = ''.join([item.residue for item in rsa_af])\n",
    "    seq_chain = ''.join([item.residue for item in rsa_chain])\n",
    "\n",
    "    for a in pairwise2.align.globalms(seq_af, seq_chain, 2, -1, -2, -1, \n",
    "                                      one_alignment_only = True, \n",
    "                                      penalize_end_gaps = False):\n",
    "        align_af = a[0]\n",
    "        align_exp = a[1]\n",
    "    \n",
    "    if len(get_gaps(align_af).mid) != 0 or len(get_gaps(align_exp).mid) != 0:\n",
    "        print('Mid gap or insertion detected. Analysis not performed')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # if there are no mid-gaps or mid-insertions continue\n",
    "    else:\n",
    "        df_af = pd.DataFrame(rsa_af, columns=('Chain_af', 'Number_af', 'Wild', 'total_abs_af', 'total_rel_af', 'main_abs_af', 'main_rel_af'))\n",
    "        df_chain = pd.DataFrame(rsa_chain, columns=('Chain_exp', 'Number_exp', 'Wild', 'total_abs_exp', 'total_rel_exp', 'main_abs_exp', 'main_rel_exp'))\n",
    "        \n",
    "        # create new common numbering\n",
    "        padding_start = max(pair.compare['gap_start'], pair.compare['insertion_start'])\n",
    "        \n",
    "        new_id_af = list(range(1 + pair.compare['insertion_start'], \n",
    "                               1 + pair.compare['num_identical'] + padding_start + pair.compare['gap_end']))\n",
    "        new_id_chain = list(range(1 + pair.compare['gap_start'], \n",
    "                                  1 + pair.compare['num_identical']  + padding_start + pair.compare['insertion_end']))\n",
    "        \n",
    "        df_af['Common_number'] = new_id_af\n",
    "        df_chain['Common_number'] = new_id_chain\n",
    "        \n",
    "        # merge results from alphafold and experimental pdb in one dataframe by joining on residue-type and common number\n",
    "        df = df_af.merge(df_chain, how = 'outer', on = ['Common_number', 'Wild'])\n",
    "        df['AF'] = pair.af_name\n",
    "        df['Experimental'] = pair.chain_name\n",
    "        df['Num_chains_exp'] = num_chains\n",
    "        df['Resolution'] = pair.res\n",
    "        df['Date'] = pair.date\n",
    "        \n",
    "        return df[['AF',\n",
    "                   'Experimental',\n",
    "                   'Resolution',\n",
    "                   'Date',\n",
    "                   'Num_chains_exp',\n",
    "                   'Common_number', \n",
    "                   'Wild', \n",
    "                   'Chain_af', \n",
    "                   'Number_af', \n",
    "                   'total_abs_af',\n",
    "                   'total_rel_af',\n",
    "                   'main_abs_af', \n",
    "                   'main_rel_af',\n",
    "                   'Chain_exp', \n",
    "                   'Number_exp',\n",
    "                   'total_abs_exp',\n",
    "                   'total_rel_exp',\n",
    "                   'main_abs_exp',\n",
    "                   'main_rel_exp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_b_factor(af):\n",
    "    '''Test that there is one pLDDT value for each residue'''\n",
    "    \n",
    "    filename = af.filename\n",
    "    structure = PDBParser(QUIET = True).get_structure('AF', '../data/external/AF_human/' + filename)\n",
    "    \n",
    "    l = []\n",
    "    for atom in structure.get_atoms():\n",
    "        l.append((atom.get_bfactor(), atom.get_parent().id[1]))\n",
    "\n",
    "    l = list(set(l))\n",
    "\n",
    "    return len(af.seq[0]) == len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pLDDT(af_filepath):\n",
    "    '''Extracts pLDDT.'''\n",
    "    \n",
    "    structure = PDBParser(QUIET = True).get_structure('AF', af_filepath)\n",
    "    \n",
    "    l = []\n",
    "    for atom in structure.get_atoms():\n",
    "        l.append((atom.get_bfactor(), atom.get_parent().id[1]))\n",
    "\n",
    "    l = list(set(l))\n",
    "\n",
    "    return [(str(item[1]), item[0]) for item in sorted(l, key = lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_atoms(pdb_code):\n",
    "    '''Checks that each residue is complete.'''\n",
    "    \n",
    "    pdb_filepath = '../data/external/PDB/pdb' + pdb_code + '.ent'\n",
    "    structure = PDBParser(QUIET = True).get_structure('pdb', pdb_filepath)\n",
    "    \n",
    "    l = []\n",
    "    for atom in structure.get_atoms():\n",
    "        if atom.get_parent().id[0] == ' ':\n",
    "            l.append(atom.get_name())\n",
    "            \n",
    "    if len(set(l)) == 1:\n",
    "        print('Only one atom-type:', l[0])\n",
    "\n",
    "    return len(set(l)) > 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-stylus",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniprotPair:\n",
    "    def __init__(self, af_uniprot, chain_uniprot, af_name, chain_name, af_seq, chain_seq):\n",
    "        self.af_uniprot = af_uniprot\n",
    "        self.chain_uniprot = chain_uniprot\n",
    "        self.af_name = af_name\n",
    "        self.chain_name = chain_name\n",
    "        self.af_seq = af_seq\n",
    "        self.chain_seq = chain_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alphafold:\n",
    "    def __init__(self, filename, uniprot, seq, fragment, version):\n",
    "        self.filename = filename\n",
    "        self.uniprot = uniprot\n",
    "        self.seq = seq\n",
    "        self.fragment = fragment\n",
    "        self.version = version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-lancaster",
   "metadata": {},
   "source": [
    "### Calculate RSA\n",
    "Load list of UniProt pair instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/uniprot_pairs.pkl', 'rb')\n",
    "uniprot_pairs = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-knitting",
   "metadata": {},
   "source": [
    "Read lists of pairs grouped according to overlap and resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-shanghai",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hi_100 = pd.read_csv('../data/group_hi_100.csv')\n",
    "hi_100 = hi_100.to_dict(orient = 'records')\n",
    "\n",
    "hi_99 = pd.read_csv('../data/group_hi_99.csv')\n",
    "hi_99 = hi_99.to_dict(orient = 'records')\n",
    "\n",
    "lo_100 = pd.read_csv('../data/group_low_100.csv')\n",
    "lo_100 = lo_100.to_dict(orient = 'records')\n",
    "\n",
    "lo_99 = pd.read_csv('../data/group_low_99.csv')\n",
    "lo_99 = lo_99.to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-equity",
   "metadata": {},
   "source": [
    "Separate UniProt pairs according to above lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-basketball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hi_100_pairs = []\n",
    "for pair in uniprot_pairs:\n",
    "    for record in hi_100:\n",
    "        if pair.af_name == record['AF'] and pair.chain_name == record['Chain']:\n",
    "            hi_100_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_99_pairs = []\n",
    "for pair in uniprot_pairs:\n",
    "    for record in hi_99:\n",
    "        if pair.af_name == record['AF'] and pair.chain_name == record['Chain']:\n",
    "            hi_99_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_100_pairs = []\n",
    "for pair in uniprot_pairs:\n",
    "    for record in lo_100:\n",
    "        if pair.af_name == record['AF'] and pair.chain_name == record['Chain']:\n",
    "            lo_100_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_99_pairs = []\n",
    "for pair in uniprot_pairs:\n",
    "    for record in lo_99:\n",
    "        if pair.af_name == record['AF'] and pair.chain_name == record['Chain']:\n",
    "            lo_99_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-alexandria",
   "metadata": {},
   "source": [
    "I'll include confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "found-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/af_human_instances_list.pkl', 'rb') \n",
    "alphafold = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-balloon",
   "metadata": {},
   "source": [
    "According to Varardi, the confidence value, pLDDT, is stored in the B-factor field in the PDBs. In the following function, I extract the b-factor (one for each atom) together with the residue-type and test that there is one unique pLDDT value for each residue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while\n",
    "all([test_b_factor(af) for af in alphafold])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-shore",
   "metadata": {},
   "source": [
    "Yes, there is. So I'll calculate RSA and extract pLDDT values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-martin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsa = []\n",
    "for pair in hi_100_pairs:\n",
    "    df = calc_RSA_pair(pair)\n",
    "    if df is not None and check_atoms(pair.chain_name[:4]):\n",
    "        plddt = pd.DataFrame(get_pLDDT('../data/external/AF_human/' + pair.af_name), columns = ['Number_af', 'pLDDT'])\n",
    "        rsa.append(df.merge(plddt, how = 'left', on = 'Number_af'))\n",
    "pd.concat(rsa).to_csv('../data/rsa_hi_100.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa = []\n",
    "for pair in hi_99_pairs:\n",
    "    df = calc_RSA_pair(pair)\n",
    "    if df is not None and check_atoms(pair.chain_name[:4]):\n",
    "        plddt = pd.DataFrame(get_pLDDT('../data/external/AF_human/' + pair.af_name), columns = ['Number_af', 'pLDDT'])\n",
    "        rsa.append(df.merge(plddt, how = 'left', on = 'Number_af'))\n",
    "pd.concat(rsa).to_csv('../data/rsa_hi_99.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa = []\n",
    "for pair in lo_100_pairs:\n",
    "    df = calc_RSA_pair(pair)\n",
    "    if df is not None and check_atoms(pair.chain_name[:4]):\n",
    "        plddt = pd.DataFrame(get_pLDDT('../data/external/AF_human/' + pair.af_name), columns = ['Number_af', 'pLDDT'])\n",
    "        rsa.append(df.merge(plddt, how = 'left', on = 'Number_af'))\n",
    "pd.concat(rsa).to_csv('../data/rsa_lo_100.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa = []\n",
    "for pair in lo_99_pairs:\n",
    "    df = calc_RSA_pair(pair)\n",
    "    if df is not None and check_atoms(pair.chain_name[:4]):\n",
    "        plddt = pd.DataFrame(get_pLDDT('../data/external/AF_human/' + pair.af_name), columns = ['Number_af', 'pLDDT'])\n",
    "        rsa.append(df.merge(plddt, how = 'left', on = 'Number_af'))\n",
    "pd.concat(rsa).to_csv('../data/rsa_lo_99.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
